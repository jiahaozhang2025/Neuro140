{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimulus-driven predictive model of brain activity: ROI-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import src.numpy_utility as pnu\n",
    "from src.file_utility import save_stuff, flatten_dict, embed_dict, zip_dict\n",
    "from src.config import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#device: 1\n",
      "device#: 0\n",
      "device name: NVIDIA GeForce RTX 2080\n",
      "\n",
      "torch: 2.6.0+cu126\n",
      "cuda:  12.6\n",
      "cudnn: 90501\n",
      "dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print ('#device:', torch.cuda.device_count())\n",
    "print ('device#:', torch.cuda.current_device())\n",
    "print ('device name:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "torch.manual_seed(time.time())\n",
    "device = torch.device(\"cuda:0\") #cuda\n",
    "torch.backends.cudnn.enabled=True\n",
    "\n",
    "print ('\\ntorch:', torch.__version__)\n",
    "print ('cuda: ', torch.version.cuda)\n",
    "print ('cudnn:', torch.backends.cudnn.version())\n",
    "print ('dtype:', torch.get_default_dtype())\n",
    "#torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Stamp: May-09-2025_1121\n"
     ]
    }
   ],
   "source": [
    "timestamp = time.strftime('%b-%d-%Y_%H%M', time.localtime()) # 'Aug-10-2020_1638' #\n",
    "\n",
    "model_name = 'gnet8r_mpf_evc'\n",
    "model_dir = '%s_%s' % (model_name, timestamp)\n",
    "\n",
    "output_dir = root_dir+\"output/multisubject/%s_%s/\" % (model_name,timestamp) \n",
    "\n",
    "trn_subjects = [1]\n",
    "for k,s in enumerate(trn_subjects): \n",
    "    subject_dir = output_dir + 'S%02d/'%s\n",
    "    if not os.path.exists(subject_dir):\n",
    "        os.makedirs(subject_dir)   \n",
    "print (\"Time Stamp: %s\" % timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the stimuli images\n",
    "From the subjectwise image preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  subject 1  -------\n",
      "block size: (10000, 3, 227, 227) , dtype: uint8 , value range: 0 255\n"
     ]
    }
   ],
   "source": [
    "exp_design = loadmat(exp_design_file)\n",
    "ordering = exp_design['masterordering'].flatten() - 1 # zero-indexed ordering of indices (matlab-like to python-like)\n",
    "\n",
    "image_data = {}\n",
    "for s in trn_subjects: \n",
    "    image_data_set = h5py.File(stim_dir + \"S%d_stimuli_227.h5py\"%s, 'r')\n",
    "    image_data[s] = np.copy(image_data_set['stimuli'])\n",
    "    image_data_set.close()\n",
    "    print ('--------  subject %d  -------' % s)\n",
    "    print ('block size:', image_data[s].shape, ', dtype:', image_data[s].dtype, ', value range:',\\\n",
    "           np.min(image_data[s][0]), np.max(image_data[s][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the voxel data\n",
    "- We load a subset (a range of voxels) of all voxel for the sake of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.file_utility import load_mask_from_nii, view_data\n",
    "from src.roi import roi_map, iterate_roi\n",
    "beta_dir = 'C:/Data/nsd/nsddata_betas/ppdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  subject 1  -------\n",
      "(81, 104, 83)\n",
      "full mask length = 699192\n",
      "selection length = 4657\n",
      "1350 \t: V1\n",
      "1433 \t: V2\n",
      "1187 \t: V3\n",
      "687 \t: hV4\n",
      "0 \t: V3ab\n",
      "0 \t: LO\n",
      "0 \t: IPS\n",
      "0 \t: VO\n",
      "0 \t: PHC\n",
      "0 \t: MT\n",
      "0 \t: MST\n",
      "0 \t: other\n"
     ]
    }
   ],
   "source": [
    "group_names = ['V1', 'V2', 'V3', 'hV4', 'V3ab', 'LO', 'IPS', 'VO', 'PHC', 'MT', 'MST', 'other']\n",
    "group = [[1,2],[3,4],[5,6], [7], [16, 17], [14, 15], [18,19,20,21,22,23], [8, 9], [10,11], [13], [12], [24,25,0]]\n",
    "\n",
    "brain_nii_shape, voxel_mask, voxel_idx, voxel_roi, voxel_ncsnr = {}, {}, {}, {}, {}\n",
    "\n",
    "for k,s in enumerate(trn_subjects):\n",
    "    print ('--------  subject %d  -------' % s)\n",
    "    voxel_mask_full = load_mask_from_nii(mask_dir + \"subj%02d/func1pt8mm/brainmask_vcventral_1.0.nii\"%s)\n",
    "    #voxel_mask_full = load_mask_from_nii(mask_dir + \"subj%02d/func1pt8mm/brainmask_nsdgeneral_1.0.nii\"%s)\n",
    "    voxel_roi_full  = load_mask_from_nii(mask_dir0 + \"subj%02d/func1pt8mm/roi/prf-visualrois.nii.gz\"%s)\n",
    "    voxel_kast_full = load_mask_from_nii(mask_dir0 + \"subj%02d/func1pt8mm/roi/Kastner2015.nii.gz\"%(s))\n",
    "    general_mask_full  = load_mask_from_nii(mask_dir0 + \"subj%02d/func1pt8mm/roi/nsdgeneral.nii.gz\"%(s))\n",
    "    ncsnr_full = load_mask_from_nii(beta_dir + \"subj%02d/func1pt8mm/betas_fithrf_GLMdenoise_RR/ncsnr.nii.gz\"%s)\n",
    "    ###\n",
    "    brain_nii_shape[s] = voxel_roi_full.shape\n",
    "    print (brain_nii_shape[s])\n",
    "    ###\n",
    "    voxel_roi_mask_full = (voxel_roi_full>0).flatten().astype(bool)\n",
    "    voxel_joined_roi_full = np.copy(voxel_kast_full.flatten())  # load kastner rois\n",
    "    voxel_joined_roi_full[voxel_roi_mask_full] = voxel_roi_full.flatten()[voxel_roi_mask_full] # overwrite with prf rois\n",
    "    ###\n",
    "    voxel_mask[s]  = np.nan_to_num(voxel_mask_full).flatten().astype(bool)\n",
    "    voxel_idx[s]   = np.arange(len(voxel_mask[s]))[voxel_mask[s]]\n",
    "    voxel_roi[s]   = voxel_joined_roi_full[voxel_mask[s]]\n",
    "    voxel_ncsnr[s] = ncsnr_full.flatten()[voxel_mask[s]]\n",
    "        \n",
    "    print ('full mask length = %d'%len(voxel_mask[s]))\n",
    "    print ('selection length = %d'%np.sum(voxel_mask[s]))\n",
    "    \n",
    "    for roi_mask, roi_name in iterate_roi(group, voxel_roi[s], roi_map, group_name=group_names):\n",
    "        print (\"%d \\t: %s\" % (np.sum(roi_mask), roi_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the batched voxel masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_group_names = ['V1', 'V2', 'V3', 'hV4'] #['V1', 'V2', 'V3', 'hV4-LO', 'IPS', 'other']\n",
    "block_group = [[1,2], [3,4], [5,6], [7]] # [[1,2], [3,4], [5,6], [7,14,15], [18,19,20,21,22,23], [8,9,10,11,12,13,16,17,24,25,0]]\n",
    "voxel_data_set = h5py.File(voxel_dir+'voxel_data_general_part1_gnet8j.h5py', 'r')\n",
    "voxel_data_dict = embed_dict({k: np.copy(d) for k,d in voxel_data_set.items()})\n",
    "voxel_data_set.close()\n",
    "voxel_data = {int(k): v for k, v in voxel_data_dict['voxel_data'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V1.1: 1350', 'V2.1: 1433', 'V3.1: 1187', 'hV4.1: 687']\n"
     ]
    }
   ],
   "source": [
    "block_voxel_masks = {}\n",
    "for roi_name in block_group_names:\n",
    "    block_voxel_masks[roi_name] = {}\n",
    "\n",
    "for s in trn_subjects:\n",
    "    for roi_mask, roi_name in iterate_roi(block_group, voxel_roi[s], roi_map, group_name=block_group_names):    \n",
    "        block_voxel_masks[roi_name][s] = roi_mask\n",
    "\n",
    "print (['%s: %d'%(s,np.sum(v)) for s,v in flatten_dict(block_voxel_masks).items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick load (Load the voxel data block for these voxels only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset split and unpacking or averaging\n",
    "## Reduce the stimulus set to those of which we have responses so far.\n",
    "- It just so happens that images with index less than 1000 are shared among subjects and are garanteed not to be shown in the remainder. We therefore chose to reparate our training and validation set along these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  subject 1  -------\n",
      "Total number of voxels = 4657\n"
     ]
    }
   ],
   "source": [
    "from src.load_nsd import image_feature_fn, ordering_split\n",
    "trn_stim_ordering, trn_voxel_data, val_stim_ordering, val_voxel_data = {},{},{},{}\n",
    "\n",
    "stim_data = {}\n",
    "data_size, nnv = {}, {}\n",
    "for k,s in enumerate(trn_subjects):\n",
    "    print ('--------  subject %d  -------' % s)\n",
    "    data_size[s], nnv[s] = voxel_data[s].shape      \n",
    "    stim_data[s] = image_feature_fn(image_data[s])\n",
    "    \n",
    "    trn_stim_ordering[s], trn_voxel_data[s], \\\n",
    "    val_stim_ordering[s], val_voxel_data[s] = \\\n",
    "        ordering_split(voxel_data[s], ordering, combine_trial=False)\n",
    "    \n",
    "del image_data\n",
    "del voxel_data\n",
    "\n",
    "trn_stim_mean = sum([np.mean(stim_data[s][1000:], axis=(0,2,3), keepdims=True) for s in trn_subjects])/len(trn_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.torch_joint_training_unpacked_sequences import *\n",
    "from src.torch_gnet3 import Encoder\n",
    "from src.torch_mpf import Torch_LayerwiseFWRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model instanciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_log_act_fn = lambda _x: T.log(1 + T.abs(_x))*T.tanh(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model_fn(_ext, _con, _x):\n",
    "    '''model consists of an extractor (_ext) and a connection model (_con)'''\n",
    "    _y, _fm, _h = _ext(_x)\n",
    "    return _con(_fm)\n",
    "\n",
    "def _smoothness_loss_fn(_rf, n):\n",
    "    delta_x = T.sum(T.pow(T.abs(_rf[:, 1:]    - _rf[:, :-1]), n))\n",
    "    delta_y = T.sum(T.pow(T.abs(_rf[:, :, 1:] - _rf[:, :, :-1]), n))\n",
    "    return delta_x + delta_y\n",
    "\n",
    "def vox_loss_fn(r, v, nu=0.5, delta=1.):\n",
    "    #err = T.sum(huber(r, v, delta), dim=0)\n",
    "    err = T.sum((r - v)**2, dim=0)\n",
    "    # squared correlation coefficient with 'leak'\n",
    "    cr = r - T.mean(r, dim=0, keepdim=True)\n",
    "    cv = v - T.mean(v, dim=0, keepdim=True)\n",
    "    wgt = T.clamp(T.pow(T.mean(cr*cv, dim=0), 2) / ((T.mean(cr**2, dim=0)) * (T.mean(cv**2, dim=0)) + 1e-6), min=nu, max=1).detach()\n",
    " \n",
    "    weighted_err = wgt * err # error per voxel\n",
    "    loss = T.sum(weighted_err) / T.mean(wgt)\n",
    "    return err, loss\n",
    "    \n",
    "def _loss_fn(_ext, _con, _x, _v):\n",
    "    _r = _model_fn(_ext, _con, _x)\n",
    "    #_err = T.sum((_r - _v)**2, dim=0)\n",
    "    #_loss = T.sum(_err)\n",
    "    _err, _loss = vox_loss_fn(_r, _v, nu=0.1, delta=.5)\n",
    "\n",
    "    _loss += fpX(1e-1) * T.sum(T.abs(_con.w))\n",
    "    return _err, _loss\n",
    "    \n",
    "def _training_fn(_ext, _con, _opts, xb, yb):\n",
    "    for _opt in _opts:\n",
    "        _opt.zero_grad()\n",
    "        _err, _loss = _loss_fn(_ext, _con, T.from_numpy(xb).to(device), T.from_numpy(yb).to(device))\n",
    "        _loss.backward()\n",
    "        _opt.step()\n",
    "    return _err\n",
    "\n",
    "def _holdout_fn(_ext, _con, xb, yb):\n",
    "    # print (xb.shape, yb.shape)\n",
    "    _err,_ = _loss_fn(_ext, _con, T.from_numpy(xb).to(device), T.from_numpy(yb).to(device))\n",
    "    return _err\n",
    "\n",
    "def _pred_fn(_ext, _con, xb):\n",
    "    return _model_fn(_ext, _con, T.from_numpy(xb).to(device))\n",
    "\n",
    "def print_grads(_ext, _con, _params, _opt, xb, yb):\n",
    "    _opt.zero_grad()\n",
    "    _err, _loss = _loss_fn(_ext, _con, T.from_numpy(xb).to(device), T.from_numpy(yb).to(device))  \n",
    "    _loss.backward()   \n",
    "    for p in _params:\n",
    "        prg = get_value(p.grad)     \n",
    "        print (\"%-16s : value=%f, grad=%f\" % (list(p.size()), np.mean(np.abs(get_value(p))), np.mean(np.abs(prg))))\n",
    "    print ('--------------------------------------')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64\n",
    "roi_nv = {s: np.sum(roi_mask) for s, roi_mask in block_voxel_masks['V1'].items()}\n",
    "    \n",
    "shared_model = Encoder(trn_stim_mean, trunk_width=N).to(device)\n",
    "rec, fmaps, h = shared_model(T.from_numpy(stim_data[trn_subjects[0]][:20]).to(device))\n",
    "\n",
    "subject_fwrfs = {s: Torch_LayerwiseFWRF(fmaps, nv=roi_nv[1], pre_nl=_log_act_fn, \\\n",
    "                     post_nl=_log_act_fn, dtype=np.float32).to(device) for s in trn_subjects}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- subject 1 ----------\n",
      "block size [1350, 27, 27]  \n",
      "block size [1350, 27, 27]  \n",
      "block size [1350, 25, 25]  \n",
      "block size [1350, 25, 25]  \n",
      "block size [1350, 25, 25]  \n",
      "block size [1350, 640]     \n",
      "block size [1350]          \n",
      "\n",
      "848064 shared params\n",
      "4602 approx params per voxels\n"
     ]
    }
   ],
   "source": [
    "for s,sp in subject_fwrfs.items():\n",
    "    print (\"--------- subject %d ----------\"%s)\n",
    "    for p in sp.parameters():\n",
    "        print (\"block size %-16s\" % (list(p.size())))\n",
    "        \n",
    "param_count = 0\n",
    "for w in shared_model.enc.parameters():\n",
    "    param_count += np.prod(tuple(w.size()))\n",
    "print ('')\n",
    "print (param_count, \"shared params\")\n",
    "total_nv = 0\n",
    "for s,sp in subject_fwrfs.items():\n",
    "    for p in sp.parameters():\n",
    "        param_count += np.prod(tuple(p.size()))\n",
    "    total_nv += roi_nv[s]\n",
    "print (param_count // total_nv, \"approx params per voxels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2031"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "T.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "***                  V1       ***\n",
      "{1: np.int64(1350)}\n",
      "***************************************\n",
      "subject 1 masked 1350 of 4657\n",
      "subject 1 training/holdout 24300 2700\n",
      "--: 100.0 %\n",
      "  Epoch 1 of 20 took       77.768s\n",
      "  training loss:               6.338067\n",
      "  holdout loss (batch):        6.117231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Codes\\nsd_gnet8x-main\\src\\torch_joint_training_unpacked_sequences.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_pred = T.tensor(val_pred).to(v.device)  # or keep in torch earlier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Subject 1: median (max) validation accuracy = 0.129 (0.410)\n",
      "** Saving params with joint score = 0.129 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 2 of 20 took       79.447s\n",
      "  training loss:               5.769326\n",
      "  holdout loss (batch):        6.048632\n",
      "  Subject 1: median (max) validation accuracy = 0.159 (0.466)\n",
      "** Saving params with joint score = 0.159 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 3 of 20 took       78.903s\n",
      "  training loss:               5.688476\n",
      "  holdout loss (batch):        5.814889\n",
      "  Subject 1: median (max) validation accuracy = 0.189 (0.529)\n",
      "** Saving params with joint score = 0.189 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 4 of 20 took       80.261s\n",
      "  training loss:               5.630840\n",
      "  holdout loss (batch):        5.748415\n",
      "  Subject 1: median (max) validation accuracy = 0.217 (0.582)\n",
      "** Saving params with joint score = 0.217 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 5 of 20 took       78.013s\n",
      "  training loss:               5.532642\n",
      "  holdout loss (batch):        5.714381\n",
      "  Subject 1: median (max) validation accuracy = 0.249 (0.646)\n",
      "** Saving params with joint score = 0.249 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 6 of 20 took       77.135s\n",
      "  training loss:               5.443814\n",
      "  holdout loss (batch):        5.572165\n",
      "  Subject 1: median (max) validation accuracy = 0.274 (0.671)\n",
      "** Saving params with joint score = 0.274 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 7 of 20 took       78.152s\n",
      "  training loss:               5.377501\n",
      "  holdout loss (batch):        5.472202\n",
      "  Subject 1: median (max) validation accuracy = 0.294 (0.688)\n",
      "** Saving params with joint score = 0.294 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 8 of 20 took       77.963s\n",
      "  training loss:               5.288190\n",
      "  holdout loss (batch):        5.483064\n",
      "  Subject 1: median (max) validation accuracy = 0.315 (0.689)\n",
      "** Saving params with joint score = 0.315 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 9 of 20 took       78.020s\n",
      "  training loss:               5.246759\n",
      "  holdout loss (batch):        5.447874\n",
      "  Subject 1: median (max) validation accuracy = 0.340 (0.711)\n",
      "** Saving params with joint score = 0.340 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 10 of 20 took       77.522s\n",
      "  training loss:               5.206273\n",
      "  holdout loss (batch):        5.383776\n",
      "  Subject 1: median (max) validation accuracy = 0.348 (0.710)\n",
      "** Saving params with joint score = 0.348 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 11 of 20 took       77.810s\n",
      "  training loss:               5.184851\n",
      "  holdout loss (batch):        5.368974\n",
      "  Subject 1: median (max) validation accuracy = 0.355 (0.711)\n",
      "** Saving params with joint score = 0.355 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 12 of 20 took       78.172s\n",
      "  training loss:               5.163713\n",
      "  holdout loss (batch):        5.343148\n",
      "  Subject 1: median (max) validation accuracy = 0.363 (0.717)\n",
      "** Saving params with joint score = 0.363 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 13 of 20 took       79.047s\n",
      "  training loss:               5.147083\n",
      "  holdout loss (batch):        5.369819\n",
      "  Subject 1: median (max) validation accuracy = 0.367 (0.715)\n",
      "** Saving params with joint score = 0.367 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 14 of 20 took       78.032s\n",
      "  training loss:               5.141138\n",
      "  holdout loss (batch):        5.265463\n",
      "  Subject 1: median (max) validation accuracy = 0.376 (0.713)\n",
      "** Saving params with joint score = 0.376 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 15 of 20 took       80.172s\n",
      "  training loss:               5.129777\n",
      "  holdout loss (batch):        5.287740\n",
      "  Subject 1: median (max) validation accuracy = 0.373 (0.716)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 16 of 20 took       78.802s\n",
      "  training loss:               5.121964\n",
      "  holdout loss (batch):        5.345481\n",
      "  Subject 1: median (max) validation accuracy = 0.381 (0.714)\n",
      "** Saving params with joint score = 0.381 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 17 of 20 took       78.584s\n",
      "  training loss:               5.100458\n",
      "  holdout loss (batch):        5.275611\n",
      "  Subject 1: median (max) validation accuracy = 0.383 (0.714)\n",
      "** Saving params with joint score = 0.383 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 18 of 20 took       77.568s\n",
      "  training loss:               5.099680\n",
      "  holdout loss (batch):        5.311144\n",
      "  Subject 1: median (max) validation accuracy = 0.380 (0.720)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 19 of 20 took       77.519s\n",
      "  training loss:               5.096961\n",
      "  holdout loss (batch):        5.274611\n",
      "  Subject 1: median (max) validation accuracy = 0.383 (0.712)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 20 of 20 took       77.262s\n",
      "  training loss:               5.079210\n",
      "  holdout loss (batch):        5.366559\n",
      "  Subject 1: median (max) validation accuracy = 0.383 (0.722)\n",
      "\n",
      "***************************************\n",
      "best joint val cc = 0.391\n",
      "subject 1: val cc = 0.391\n",
      "***************************************\n",
      "***************************************\n",
      "***                  V2       ***\n",
      "{1: np.int64(1433)}\n",
      "***************************************\n",
      "subject 1 masked 1433 of 4657\n",
      "subject 1 training/holdout 24300 2700\n",
      "--: 100.0 %\n",
      "  Epoch 1 of 20 took       80.265s\n",
      "  training loss:               6.124651\n",
      "  holdout loss (batch):        5.857065\n",
      "  Subject 1: median (max) validation accuracy = 0.070 (0.333)\n",
      "** Saving params with joint score = 0.070 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 2 of 20 took       79.944s\n",
      "  training loss:               5.745672\n",
      "  holdout loss (batch):        5.805803\n",
      "  Subject 1: median (max) validation accuracy = 0.089 (0.395)\n",
      "** Saving params with joint score = 0.089 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 3 of 20 took       79.679s\n",
      "  training loss:               5.679955\n",
      "  holdout loss (batch):        5.700433\n",
      "  Subject 1: median (max) validation accuracy = 0.128 (0.526)\n",
      "** Saving params with joint score = 0.128 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 4 of 20 took       79.745s\n",
      "  training loss:               5.619782\n",
      "  holdout loss (batch):        5.673886\n",
      "  Subject 1: median (max) validation accuracy = 0.167 (0.585)\n",
      "** Saving params with joint score = 0.167 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 5 of 20 took       79.800s\n",
      "  training loss:               5.550570\n",
      "  holdout loss (batch):        5.644137\n",
      "  Subject 1: median (max) validation accuracy = 0.195 (0.607)\n",
      "** Saving params with joint score = 0.195 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 6 of 20 took       79.772s\n",
      "  training loss:               5.502694\n",
      "  holdout loss (batch):        5.550700\n",
      "  Subject 1: median (max) validation accuracy = 0.216 (0.621)\n",
      "** Saving params with joint score = 0.216 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 7 of 20 took       79.692s\n",
      "  training loss:               5.475923\n",
      "  holdout loss (batch):        5.537217\n",
      "  Subject 1: median (max) validation accuracy = 0.225 (0.620)\n",
      "** Saving params with joint score = 0.225 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 8 of 20 took       79.686s\n",
      "  training loss:               5.447175\n",
      "  holdout loss (batch):        5.530551\n",
      "  Subject 1: median (max) validation accuracy = 0.236 (0.616)\n",
      "** Saving params with joint score = 0.236 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 9 of 20 took       79.643s\n",
      "  training loss:               5.433918\n",
      "  holdout loss (batch):        5.486531\n",
      "  Subject 1: median (max) validation accuracy = 0.249 (0.636)\n",
      "** Saving params with joint score = 0.249 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 10 of 20 took       80.087s\n",
      "  training loss:               5.414156\n",
      "  holdout loss (batch):        5.454086\n",
      "  Subject 1: median (max) validation accuracy = 0.260 (0.640)\n",
      "** Saving params with joint score = 0.260 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 11 of 20 took       79.849s\n",
      "  training loss:               5.402713\n",
      "  holdout loss (batch):        5.462491\n",
      "  Subject 1: median (max) validation accuracy = 0.265 (0.641)\n",
      "** Saving params with joint score = 0.265 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 12 of 20 took       79.864s\n",
      "  training loss:               5.388303\n",
      "  holdout loss (batch):        5.435887\n",
      "  Subject 1: median (max) validation accuracy = 0.269 (0.642)\n",
      "** Saving params with joint score = 0.269 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 13 of 20 took       79.740s\n",
      "  training loss:               5.373462\n",
      "  holdout loss (batch):        5.412235\n",
      "  Subject 1: median (max) validation accuracy = 0.275 (0.644)\n",
      "** Saving params with joint score = 0.275 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 14 of 20 took       79.922s\n",
      "  training loss:               5.362972\n",
      "  holdout loss (batch):        5.538374\n",
      "  Subject 1: median (max) validation accuracy = 0.278 (0.646)\n",
      "** Saving params with joint score = 0.278 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 15 of 20 took       79.803s\n",
      "  training loss:               5.368196\n",
      "  holdout loss (batch):        5.426167\n",
      "  Subject 1: median (max) validation accuracy = 0.277 (0.632)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 16 of 20 took       79.775s\n",
      "  training loss:               5.356112\n",
      "  holdout loss (batch):        5.414229\n",
      "  Subject 1: median (max) validation accuracy = 0.287 (0.652)\n",
      "** Saving params with joint score = 0.287 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 17 of 20 took       80.081s\n",
      "  training loss:               5.346676\n",
      "  holdout loss (batch):        5.417369\n",
      "  Subject 1: median (max) validation accuracy = 0.288 (0.649)\n",
      "** Saving params with joint score = 0.288 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 18 of 20 took       80.089s\n",
      "  training loss:               5.339890\n",
      "  holdout loss (batch):        5.393141\n",
      "  Subject 1: median (max) validation accuracy = 0.289 (0.654)\n",
      "** Saving params with joint score = 0.289 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 19 of 20 took       79.967s\n",
      "  training loss:               5.333275\n",
      "  holdout loss (batch):        5.404392\n",
      "  Subject 1: median (max) validation accuracy = 0.288 (0.655)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 20 of 20 took       79.808s\n",
      "  training loss:               5.327728\n",
      "  holdout loss (batch):        5.359897\n",
      "  Subject 1: median (max) validation accuracy = 0.294 (0.661)\n",
      "** Saving params with joint score = 0.294 **\n",
      "\n",
      "***************************************\n",
      "best joint val cc = 0.311\n",
      "subject 1: val cc = 0.311\n",
      "***************************************\n",
      "***************************************\n",
      "***                  V3       ***\n",
      "{1: np.int64(1187)}\n",
      "***************************************\n",
      "subject 1 masked 1187 of 4657\n",
      "subject 1 training/holdout 24300 2700\n",
      "--: 100.0 %\n",
      "  Epoch 1 of 20 took       73.975s\n",
      "  training loss:               4.059884\n",
      "  holdout loss (batch):        3.694430\n",
      "  Subject 1: median (max) validation accuracy = 0.049 (0.210)\n",
      "** Saving params with joint score = 0.049 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 2 of 20 took       73.601s\n",
      "  training loss:               3.709415\n",
      "  holdout loss (batch):        3.798226\n",
      "  Subject 1: median (max) validation accuracy = 0.088 (0.342)\n",
      "** Saving params with joint score = 0.088 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 3 of 20 took       73.384s\n",
      "  training loss:               3.676864\n",
      "  holdout loss (batch):        3.766111\n",
      "  Subject 1: median (max) validation accuracy = 0.106 (0.376)\n",
      "** Saving params with joint score = 0.106 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 4 of 20 took       73.492s\n",
      "  training loss:               3.642219\n",
      "  holdout loss (batch):        3.696551\n",
      "  Subject 1: median (max) validation accuracy = 0.138 (0.403)\n",
      "** Saving params with joint score = 0.138 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 5 of 20 took       73.502s\n",
      "  training loss:               3.602066\n",
      "  holdout loss (batch):        3.841377\n",
      "  Subject 1: median (max) validation accuracy = 0.167 (0.441)\n",
      "** Saving params with joint score = 0.167 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 6 of 20 took       73.669s\n",
      "  training loss:               3.565586\n",
      "  holdout loss (batch):        3.590199\n",
      "  Subject 1: median (max) validation accuracy = 0.170 (0.445)\n",
      "** Saving params with joint score = 0.170 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 7 of 20 took       74.283s\n",
      "  training loss:               3.548406\n",
      "  holdout loss (batch):        3.537243\n",
      "  Subject 1: median (max) validation accuracy = 0.195 (0.474)\n",
      "** Saving params with joint score = 0.195 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 8 of 20 took       74.026s\n",
      "  training loss:               3.522650\n",
      "  holdout loss (batch):        3.528334\n",
      "  Subject 1: median (max) validation accuracy = 0.206 (0.482)\n",
      "** Saving params with joint score = 0.206 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 9 of 20 took       74.908s\n",
      "  training loss:               3.512176\n",
      "  holdout loss (batch):        3.516212\n",
      "  Subject 1: median (max) validation accuracy = 0.215 (0.502)\n",
      "** Saving params with joint score = 0.215 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 10 of 20 took       75.390s\n",
      "  training loss:               3.503572\n",
      "  holdout loss (batch):        3.577110\n",
      "  Subject 1: median (max) validation accuracy = 0.217 (0.501)\n",
      "** Saving params with joint score = 0.217 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 11 of 20 took       75.184s\n",
      "  training loss:               3.491123\n",
      "  holdout loss (batch):        3.556283\n",
      "  Subject 1: median (max) validation accuracy = 0.219 (0.504)\n",
      "** Saving params with joint score = 0.219 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 12 of 20 took       74.113s\n",
      "  training loss:               3.475605\n",
      "  holdout loss (batch):        3.518798\n",
      "  Subject 1: median (max) validation accuracy = 0.234 (0.522)\n",
      "** Saving params with joint score = 0.234 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 13 of 20 took       73.462s\n",
      "  training loss:               3.463778\n",
      "  holdout loss (batch):        3.626614\n",
      "  Subject 1: median (max) validation accuracy = 0.241 (0.523)\n",
      "** Saving params with joint score = 0.241 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 14 of 20 took       74.097s\n",
      "  training loss:               3.455683\n",
      "  holdout loss (batch):        3.452427\n",
      "  Subject 1: median (max) validation accuracy = 0.237 (0.527)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 15 of 20 took       73.623s\n",
      "  training loss:               3.454318\n",
      "  holdout loss (batch):        3.566408\n",
      "  Subject 1: median (max) validation accuracy = 0.238 (0.523)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 16 of 20 took       73.440s\n",
      "  training loss:               3.450130\n",
      "  holdout loss (batch):        3.463262\n",
      "  Subject 1: median (max) validation accuracy = 0.250 (0.536)\n",
      "** Saving params with joint score = 0.250 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 17 of 20 took       73.121s\n",
      "  training loss:               3.440682\n",
      "  holdout loss (batch):        3.451886\n",
      "  Subject 1: median (max) validation accuracy = 0.248 (0.535)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 18 of 20 took       73.394s\n",
      "  training loss:               3.436450\n",
      "  holdout loss (batch):        3.476407\n",
      "  Subject 1: median (max) validation accuracy = 0.255 (0.545)\n",
      "** Saving params with joint score = 0.255 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 19 of 20 took       73.223s\n",
      "  training loss:               3.434288\n",
      "  holdout loss (batch):        3.501416\n",
      "  Subject 1: median (max) validation accuracy = 0.254 (0.540)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 20 of 20 took       73.171s\n",
      "  training loss:               3.416843\n",
      "  holdout loss (batch):        3.462172\n",
      "  Subject 1: median (max) validation accuracy = 0.257 (0.552)\n",
      "** Saving params with joint score = 0.257 **\n",
      "\n",
      "***************************************\n",
      "best joint val cc = 0.268\n",
      "subject 1: val cc = 0.268\n",
      "***************************************\n",
      "***************************************\n",
      "***                 hV4       ***\n",
      "{1: np.int64(687)}\n",
      "***************************************\n",
      "subject 1 masked 687 of 4657\n",
      "subject 1 training/holdout 24300 2700\n",
      "--: 100.0 %\n",
      "  Epoch 1 of 20 took       60.349s\n",
      "  training loss:               4.432207\n",
      "  holdout loss (batch):        4.005965\n",
      "  Subject 1: median (max) validation accuracy = 0.060 (0.223)\n",
      "** Saving params with joint score = 0.060 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 2 of 20 took       60.157s\n",
      "  training loss:               4.044728\n",
      "  holdout loss (batch):        3.966660\n",
      "  Subject 1: median (max) validation accuracy = 0.071 (0.312)\n",
      "** Saving params with joint score = 0.071 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 3 of 20 took       60.421s\n",
      "  training loss:               4.018071\n",
      "  holdout loss (batch):        3.940309\n",
      "  Subject 1: median (max) validation accuracy = 0.080 (0.319)\n",
      "** Saving params with joint score = 0.080 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 4 of 20 took       59.934s\n",
      "  training loss:               4.007766\n",
      "  holdout loss (batch):        4.011598\n",
      "  Subject 1: median (max) validation accuracy = 0.095 (0.395)\n",
      "** Saving params with joint score = 0.095 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 5 of 20 took       60.014s\n",
      "  training loss:               3.983123\n",
      "  holdout loss (batch):        4.042270\n",
      "  Subject 1: median (max) validation accuracy = 0.102 (0.345)\n",
      "** Saving params with joint score = 0.102 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 6 of 20 took       60.037s\n",
      "  training loss:               3.962626\n",
      "  holdout loss (batch):        3.872068\n",
      "  Subject 1: median (max) validation accuracy = 0.123 (0.435)\n",
      "** Saving params with joint score = 0.123 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 7 of 20 took       60.061s\n",
      "  training loss:               3.944273\n",
      "  holdout loss (batch):        4.000680\n",
      "  Subject 1: median (max) validation accuracy = 0.137 (0.465)\n",
      "** Saving params with joint score = 0.137 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 8 of 20 took       60.014s\n",
      "  training loss:               3.931466\n",
      "  holdout loss (batch):        3.834818\n",
      "  Subject 1: median (max) validation accuracy = 0.143 (0.474)\n",
      "** Saving params with joint score = 0.143 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 9 of 20 took       60.114s\n",
      "  training loss:               3.906775\n",
      "  holdout loss (batch):        3.934262\n",
      "  Subject 1: median (max) validation accuracy = 0.145 (0.458)\n",
      "** Saving params with joint score = 0.145 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 10 of 20 took       60.097s\n",
      "  training loss:               3.902505\n",
      "  holdout loss (batch):        3.895916\n",
      "  Subject 1: median (max) validation accuracy = 0.152 (0.487)\n",
      "** Saving params with joint score = 0.152 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 11 of 20 took       59.740s\n",
      "  training loss:               3.885439\n",
      "  holdout loss (batch):        3.833494\n",
      "  Subject 1: median (max) validation accuracy = 0.158 (0.485)\n",
      "** Saving params with joint score = 0.158 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 12 of 20 took       60.201s\n",
      "  training loss:               3.873545\n",
      "  holdout loss (batch):        3.829549\n",
      "  Subject 1: median (max) validation accuracy = 0.163 (0.505)\n",
      "** Saving params with joint score = 0.163 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 13 of 20 took       62.499s\n",
      "  training loss:               3.878893\n",
      "  holdout loss (batch):        3.774194\n",
      "  Subject 1: median (max) validation accuracy = 0.159 (0.491)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 14 of 20 took       61.445s\n",
      "  training loss:               3.861353\n",
      "  holdout loss (batch):        3.915186\n",
      "  Subject 1: median (max) validation accuracy = 0.162 (0.498)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 15 of 20 took       61.400s\n",
      "  training loss:               3.859748\n",
      "  holdout loss (batch):        3.819345\n",
      "  Subject 1: median (max) validation accuracy = 0.166 (0.518)\n",
      "** Saving params with joint score = 0.166 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 16 of 20 took       61.724s\n",
      "  training loss:               3.853706\n",
      "  holdout loss (batch):        3.805908\n",
      "  Subject 1: median (max) validation accuracy = 0.165 (0.515)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 17 of 20 took       64.688s\n",
      "  training loss:               3.853858\n",
      "  holdout loss (batch):        3.789590\n",
      "  Subject 1: median (max) validation accuracy = 0.166 (0.532)\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 18 of 20 took       60.002s\n",
      "  training loss:               3.835363\n",
      "  holdout loss (batch):        3.795321\n",
      "  Subject 1: median (max) validation accuracy = 0.169 (0.511)\n",
      "** Saving params with joint score = 0.169 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 19 of 20 took       60.295s\n",
      "  training loss:               3.828071\n",
      "  holdout loss (batch):        3.845641\n",
      "  Subject 1: median (max) validation accuracy = 0.171 (0.513)\n",
      "** Saving params with joint score = 0.171 **\n",
      "\n",
      "--: 100.0 %\n",
      "  Epoch 20 of 20 took       59.797s\n",
      "  training loss:               3.825628\n",
      "  holdout loss (batch):        3.841897\n",
      "  Subject 1: median (max) validation accuracy = 0.177 (0.540)\n",
      "** Saving params with joint score = 0.177 **\n",
      "\n",
      "***************************************\n",
      "best joint val cc = 0.183\n",
      "subject 1: val cc = 0.183\n",
      "***************************************\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "holdout_frac = .1\n",
    "\n",
    "for roi_name, roi_masks in block_voxel_masks.items(): \n",
    "    roi_nv = {s: np.sum(roi_mask) for s, roi_mask in roi_masks.items()}\n",
    "    \n",
    "    print ('***************************************')\n",
    "    print ('***        %12s       ***'%roi_name)\n",
    "    print ({s: np.sum(m) for s,m in roi_masks.items()})\n",
    "    print ('***************************************') \n",
    "    \n",
    "    # create model\n",
    "    shared_model = Encoder(trn_stim_mean, trunk_width=N).to(device)\n",
    "    rec, fmaps, h = shared_model(T.from_numpy(stim_data[trn_subjects[0]][:20]).to(device))\n",
    "\n",
    "    subject_fwrfs = {s: Torch_LayerwiseFWRF(fmaps, nv=roi_nv[s], pre_nl=_log_act_fn, \\\n",
    "                     post_nl=_log_act_fn, dtype=np.float32).to(device) for s in trn_subjects}    \n",
    "    \n",
    "    ##################################\n",
    "    try:\n",
    "        from torch.hub import load_state_dict_from_url\n",
    "    except ImportError:\n",
    "        from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "\n",
    "    state_dict = load_state_dict_from_url('https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth', progress=True)\n",
    "            ### Rename dictionary keys to match new breakdown\n",
    "    '''pre_state_dict = {}\n",
    "    pre_state_dict['conv1.0.weight'] = state_dict.pop('features.0.weight')\n",
    "    pre_state_dict['conv1.0.bias'] = state_dict.pop('features.0.bias')\n",
    "    pre_state_dict['conv2.0.weight'] = state_dict.pop('features.3.weight')\n",
    "    pre_state_dict['conv2.0.bias'] = state_dict.pop('features.3.bias')\n",
    "\n",
    "    shared_model.pre.load_state_dict(pre_state_dict)'''\n",
    "    ###################################\n",
    "    optimizer_net = optim.Adam([\n",
    "            #{'params': shared_model.pre.parameters()},\n",
    "            {'params': shared_model.enc.parameters()},\n",
    "        ], lr=1e-3, betas=(0.9, 0.999), eps=1e-08)\n",
    "    subject_optimizer = {s: optim.Adam([\n",
    "            {'params': sp.parameters()}\n",
    "        ], lr=1e-3, betas=(0.9, 0.999), eps=1e-08) for s,sp in subject_fwrfs.items()}\n",
    "    subject_opts = {s: [optimizer_net, subject_optimizer[s]] for s in subject_optimizer.keys()}   \n",
    "    ###################################\n",
    "    best_params, final_params, hold_cc_hist, hold_hist, trn_hist, best_epoch, best_joint_cc_score = \\\n",
    "        learn_params_(_training_fn, _holdout_fn, _pred_fn, shared_model, subject_fwrfs, subject_opts, \n",
    "            stim_data, trn_voxel_data, trn_stim_ordering,\n",
    "            num_epochs=num_epochs, batch_size=batch_size, holdout_frac=holdout_frac, masks=roi_masks, randomize=False)\n",
    "    \n",
    "    ###################################\n",
    "    #val_voxel = {s: val_voxel_data[s] for s in val_voxel_data.keys()}\n",
    "    shared_model.load_state_dict(best_params['enc'])\n",
    "    shared_model.eval() \n",
    "    for s,sd in subject_fwrfs.items():\n",
    "        sd.load_state_dict(best_params['fwrfs'][s])\n",
    "        sd.eval() \n",
    "\n",
    "    subject_val_cc = validation_(_pred_fn, shared_model, subject_fwrfs, stim_data, val_voxel_data, val_stim_ordering, batch_size, masks=roi_masks)\n",
    "    joined_val_cc = np.concatenate(list(subject_val_cc.values()), axis=0)\n",
    "    \n",
    "    print ('***************************************')\n",
    "    print (\"best joint val cc = %.3f\"% np.median(joined_val_cc))\n",
    "    for s,v in subject_val_cc.items():\n",
    "        print (\"subject %s: val cc = %.3f\"%(s, np.median(v)))\n",
    " \n",
    "    print ('***************************************')\n",
    "    ###################################\n",
    "    torch.save({\n",
    "            'group_name': roi_name,\n",
    "            'num_epochs': num_epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'holdout_frac': holdout_frac,   \n",
    "            'best_params': best_params, \n",
    "            'final_params': final_params,\n",
    "            'trn_loss_history': trn_hist,\n",
    "            'hold_loss_history': hold_hist,\n",
    "            'hold_cc_history': hold_cc_hist,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_joint_cc_score': best_joint_cc_score,\n",
    "            'val_cc': subject_val_cc,\n",
    "            'input_mean': trn_stim_mean,\n",
    "            'brain_nii_shape': brain_nii_shape,\n",
    "            'voxel_index': voxel_idx,\n",
    "            'voxel_roi': voxel_roi,\n",
    "            'voxel_mask': voxel_mask,\n",
    "            'group_mask': roi_masks\n",
    "            }, output_dir+'model_params_%s'%roi_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-ROI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir  = output_dir\n",
    "subset_info='3_all_epoch20'\n",
    "roi_files = {\n",
    "    'V1': '%smodel_params_V1_%s'%(input_dir,subset_info),\n",
    "    'V2': '%smodel_params_V2_%s'%(input_dir,subset_info),\n",
    "    'V3': '%smodel_params_V3_%s'%(input_dir,subset_info),\n",
    "    'hV4': '%smodel_params_hV4_%s'%(input_dir,subset_info)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Data/nsd_test/output/multisubject/gnet8r_mpf_evc_May-09-2025_1121/V1/\n",
      "***************************************\n",
      "***                  V1       ***\n",
      "from ==> C:/Data/nsd_test/output/multisubject/gnet8r_mpf_evc_May-09-2025_1121/model_params_V1_3_all_epoch20\n",
      "to   ==> C:/Data/nsd_test/output/multisubject/gnet8r_mpf_evc_May-09-2025_1121/V1/\n",
      "***************************************\n",
      "subject 1 masked 4657 of 4657\n",
      "subject 1 training/holdout 24300 2700\n",
      "| : 0.4 %"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 42\u001b[0m\n\u001b[0;32m     37\u001b[0m subject_opts \u001b[38;5;241m=\u001b[39m {s: [subject_optimizer[s]] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m subject_optimizer\u001b[38;5;241m.\u001b[39mkeys()}   \n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m###################################\u001b[39;00m\n\u001b[0;32m     41\u001b[0m best_params, final_params, hold_cc_hist, hold_hist, trn_hist, best_epoch, best_joint_cc_score \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mlearn_params_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_training_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_holdout_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pred_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject_fwrfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject_opts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstim_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrn_voxel_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrn_stim_ordering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m###################################\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#val_voxel = {s: val_voxel_data[s] for s in val_voxel_data.keys()}\u001b[39;00m\n\u001b[0;32m     49\u001b[0m shared_model\u001b[38;5;241m.\u001b[39mload_state_dict(best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\Codes\\nsd_gnet8x-main\\src\\torch_joint_training_unpacked_sequences.py:193\u001b[0m, in \u001b[0;36mlearn_params_\u001b[1;34m(_trn_fn, _hld_fn, _pred_fn, _ext, _cons, _opts, stims, voxels, ordering, num_epochs, batch_size, holdout_frac, trn_size, masks, randomize)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s,_c \u001b[38;5;129;01min\u001b[39;00m _cons\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    192\u001b[0m     _c\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 193\u001b[0m trn_err \u001b[38;5;241m=\u001b[39m \u001b[43msubject_training_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_trn_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_cons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_opts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrn_voxels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrn_stim_ordering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m trn_hist \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [trn_err,]\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m##\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Codes\\nsd_gnet8x-main\\src\\torch_joint_training_unpacked_sequences.py:106\u001b[0m, in \u001b[0;36msubject_training_pass\u001b[1;34m(_trn_fn, _ext, _cons, _ops, x, v, ordering, batch_size)\u001b[0m\n\u001b[0;32m    104\u001b[0m trn_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, xb, vb \u001b[38;5;129;01min\u001b[39;00m iterate_subject_ordering_minibatches(x, v, ordering, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 106\u001b[0m     trn_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m get_value(T\u001b[38;5;241m.\u001b[39mmean(\u001b[43m_trn_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_cons\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ops\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvb\u001b[49m\u001b[43m)\u001b[49m)) \n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trn_err \u001b[38;5;241m/\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vv) \u001b[38;5;28;01mfor\u001b[39;00m s,vv \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mitems())\n",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m, in \u001b[0;36m_training_fn\u001b[1;34m(_ext, _con, _opts, xb, yb)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _opt \u001b[38;5;129;01min\u001b[39;00m _opts:\n\u001b[0;32m     34\u001b[0m     _opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 35\u001b[0m     _err, _loss \u001b[38;5;241m=\u001b[39m \u001b[43m_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_con\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     _loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     37\u001b[0m     _opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[13], line 24\u001b[0m, in \u001b[0;36m_loss_fn\u001b[1;34m(_ext, _con, _x, _v)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_loss_fn\u001b[39m(_ext, _con, _x, _v):\n\u001b[1;32m---> 24\u001b[0m     _r \u001b[38;5;241m=\u001b[39m \u001b[43m_model_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_con\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m#_err = T.sum((_r - _v)**2, dim=0)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#_loss = T.sum(_err)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     _err, _loss \u001b[38;5;241m=\u001b[39m vox_loss_fn(_r, _v, nu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m, in \u001b[0;36m_model_fn\u001b[1;34m(_ext, _con, _x)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''model consists of an extractor (_ext) and a connection model (_con)'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m _y, _fm, _h \u001b[38;5;241m=\u001b[39m _ext(_x)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_con\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\neuro\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\neuro\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Codes\\nsd_gnet8x-main\\src\\torch_mpf.py:151\u001b[0m, in \u001b[0;36mTorch_LayerwiseFWRF.forward\u001b[1;34m(self, fmaps)\u001b[0m\n\u001b[0;32m    149\u001b[0m Phi \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mcat(phi, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_nl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     Phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_nl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPhi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m vr \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39msqueeze(T\u001b[38;5;241m.\u001b[39mbmm(Phi, T\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw,\u001b[38;5;241m2\u001b[39m)))\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m+\u001b[39m T\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb,\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vr\n",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(_x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _log_act_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m _x: \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mT\u001b[38;5;241m.\u001b[39mtanh(_x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "holdout_frac = .1\n",
    "\n",
    "for roi_name, roi_file in roi_files.items(): \n",
    "\n",
    "    # create subdir\n",
    "    model_dir     = roi_file\n",
    "    output_subdir = output_dir + roi_name + '/'\n",
    "    print (output_subdir)\n",
    "    if not os.path.exists(output_subdir):\n",
    "        os.makedirs(output_subdir)\n",
    "\n",
    "    print ('***************************************')\n",
    "    print ('***        %12s       ***'%roi_name)\n",
    "    print ('from ==> %s'%model_dir)\n",
    "    print ('to   ==> %s'%output_subdir)\n",
    "    print ('***************************************') \n",
    "    \n",
    "    \n",
    "    checkpoint = torch.load(model_dir,weights_only=False)\n",
    "    shared_params = checkpoint['best_params']\n",
    "    ###\n",
    "    \n",
    "    shared_model = Encoder(trn_stim_mean, trunk_width=N).to(device)\n",
    "    shared_model.load_state_dict(shared_params['enc'])\n",
    "    shared_model.eval()     \n",
    "\n",
    "    rec, fmaps, h = shared_model(T.from_numpy(stim_data[list(stim_data.keys())[0]][:20]).to(device))\n",
    "    \n",
    "    subject_fwrfs = {s: Torch_LayerwiseFWRF(fmaps, nv=trn_voxel_data[s].shape[1], pre_nl=_log_act_fn, \\\n",
    "                     post_nl=_log_act_fn, dtype=np.float32).to(device) for s in trn_subjects}    \n",
    "    \n",
    "    subject_optimizer = {s: optim.Adam([\n",
    "            {'params': sp.parameters()}\n",
    "        ], lr=1e-3, betas=(0.9, 0.999), eps=1e-08) for s,sp in subject_fwrfs.items()}\n",
    "    subject_opts = {s: [subject_optimizer[s]] for s in subject_optimizer.keys()}   \n",
    "    ###################################\n",
    "    \n",
    "\n",
    "    best_params, final_params, hold_cc_hist, hold_hist, trn_hist, best_epoch, best_joint_cc_score = \\\n",
    "        learn_params_(_training_fn, _holdout_fn, _pred_fn, shared_model, subject_fwrfs, subject_opts, \n",
    "            stim_data, trn_voxel_data, trn_stim_ordering,\n",
    "            num_epochs=num_epochs, batch_size=batch_size, holdout_frac=holdout_frac, randomize=False)\n",
    "    \n",
    "    \n",
    "    ###################################\n",
    "    #val_voxel = {s: val_voxel_data[s] for s in val_voxel_data.keys()}\n",
    "    shared_model.load_state_dict(best_params['enc'])\n",
    "    shared_model.eval() \n",
    "    for s,sd in subject_fwrfs.items():\n",
    "        sd.load_state_dict(best_params['fwrfs'][s])\n",
    "        sd.eval() \n",
    "\n",
    "    subject_val_cc = validation_(_pred_fn, shared_model, subject_fwrfs, stim_data, val_voxel_data, val_stim_ordering, batch_size)\n",
    "    joined_val_cc = np.concatenate(list(subject_val_cc.values()), axis=0)\n",
    "    \n",
    "    print ('***************************************')\n",
    "    print (\"best joint val cc = %.3f\"% np.median(joined_val_cc))\n",
    "    for s,v in subject_val_cc.items():\n",
    "        print (\"subject %s: val cc = %.3f\"%(s, np.median(v)))\n",
    " \n",
    "    print ('***************************************')\n",
    "    ###################################\n",
    "    torch.save({\n",
    "            'group_name': roi_name,\n",
    "            'num_epochs': num_epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'holdout_frac': holdout_frac,   \n",
    "            'best_params': best_params, \n",
    "            'final_params': final_params,\n",
    "            'trn_loss_history': trn_hist,\n",
    "            'hold_loss_history': hold_hist,\n",
    "            'hold_cc_history': hold_cc_hist,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_joint_cc_score': best_joint_cc_score,\n",
    "            'val_cc': subject_val_cc,\n",
    "            'input_mean': trn_stim_mean,\n",
    "            'brain_nii_shape': brain_nii_shape,\n",
    "            'voxel_index': voxel_idx,\n",
    "            'voxel_roi': voxel_roi,\n",
    "            'voxel_mask': voxel_mask,\n",
    "            }, output_subdir+'model_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
